{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af45c499-a55e-4fde-a5e8-0491ce4bf696",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066166b3-d6eb-49cf-88d2-57f37f085c5e",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889dbfa3-9e15-46d4-b9b5-c15ea304fb33",
   "metadata": {},
   "source": [
    "## Import library and set directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60de0fe-66b8-4a8c-b818-9e8695f74de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "# Add the path to the designated folder containing custom modules.\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import the custom module for benthic habitat mapping.\n",
    "# This module contains functions and utilities for tasks such as\n",
    "# correction and classification of benthic habitats.\n",
    "import benthic_mapping as bm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('dark_background')\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb059541-3e24-4d56-8546-f14448a03729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_file_path(out_dir, user_year):\n",
    "    # Ensure the year is a string\n",
    "    user_year = str(user_year)\n",
    "    # Construct the directory path\n",
    "    year_folder = os.path.join(out_dir, 'atmospheric_correction', user_year)\n",
    "    # Iterate through the files in the year folder\n",
    "    for filename in os.listdir(year_folder):\n",
    "        # Check if the file contains 'L2R' in its name\n",
    "        if 'L2R' in filename:\n",
    "            # Construct the full file path\n",
    "            file_data_path = os.path.join(year_folder, filename)\n",
    "            return file_data_path\n",
    "    # If no file is found, return None\n",
    "    return\n",
    "\n",
    "# Define folder path\n",
    "base_dir =  os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "out_dir = os.path.join(base_dir, 'out')\n",
    "raw_data_dir = os.path.join(data_dir, 'raw')\n",
    "\n",
    "# Define file paths\n",
    "user_year = \"default\"\n",
    "file_data_path = construct_file_path(out_dir, user_year)\n",
    "shapefile_path_deepWater = os.path.join(out_dir, 'geom_def', 'geom_deepWater.shp')\n",
    "shapefile_path_sandObject = os.path.join(out_dir, 'geom_def', 'geom_sandObject.shp')\n",
    "shapefile_path_training_1 = os.path.join(out_dir, 'geom_def', 'geom_landWater.shp')\n",
    "shapefile_path_training_2 = os.path.join(out_dir, 'geom_def', 'geom_benthicObject.shp')\n",
    "\n",
    "# Extract the base filename\n",
    "base_filename = os.path.splitext(os.path.basename(file_data_path))[0]\n",
    "base_filename = base_filename.rsplit('_', 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df733fa-5483-46ec-a49f-d0136ca14479",
   "metadata": {},
   "source": [
    "## Pre-processing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b54cb0",
   "metadata": {},
   "source": [
    "### Open and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe17414-cc82-4cd1-9392-94aa9ed1a65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected dataset from Sentinel-2B\n"
     ]
    }
   ],
   "source": [
    "# Open dataset\n",
    "data = xr.open_dataset(file_data_path)\n",
    "\n",
    "# Determine if the dataset is from S2A or S2B based on the filename\n",
    "if 'S2A' in file_data_path:\n",
    "    print('Detected dataset from Sentinel-2A')\n",
    "    variables_to_keep = {\n",
    "        'transverse_mercator': 'transverse_mercator',\n",
    "        'lat': 'lat',\n",
    "        'lon': 'lon',\n",
    "        'rhos_492': 'blue',\n",
    "        'rhos_560': 'green',\n",
    "        'rhos_665': 'red',\n",
    "        'rhos_704': 'red_edge',\n",
    "        'rhos_833': 'nir',\n",
    "        'rhos_1614': 'swir1',\n",
    "        'rhos_2202': 'swir2'\n",
    "    }\n",
    "elif 'S2B' in file_data_path:\n",
    "    print('Detected dataset from Sentinel-2B')\n",
    "    variables_to_keep = {\n",
    "        'transverse_mercator': 'transverse_mercator',\n",
    "        'lat': 'lat',\n",
    "        'lon': 'lon',\n",
    "        'rhos_492': 'blue',\n",
    "        'rhos_559': 'green',\n",
    "        'rhos_665': 'red',\n",
    "        'rhos_704': 'red_edge',\n",
    "        'rhos_833': 'nir',\n",
    "        'rhos_1610': 'swir1',\n",
    "        'rhos_2186': 'swir2'\n",
    "    }\n",
    "else:\n",
    "    raise ValueError(\"The dataset file path does not indicate whether it is S2A or S2B.\")\n",
    "\n",
    "# Create the new dataset\n",
    "new_vars = {}\n",
    "for old_name, new_name in variables_to_keep.items():\n",
    "    if old_name in data:\n",
    "        # Select the variable and transpose if needed\n",
    "        variable = data[old_name]\n",
    "        new_vars[new_name] = variable\n",
    "\n",
    "# Construct the new dataset\n",
    "ds = xr.Dataset(new_vars)\n",
    "\n",
    "# Preserve selected attributes\n",
    "attributes_to_keep = [\n",
    "    'generated_by', 'generated_on', 'contact', 'product_type', 'metadata_profile', 'Conventions', \n",
    "    'sensor', 'isodate', 'global_dims', 'sza', 'vza', 'raa', 'scene_xrange', 'scene_yrange', \n",
    "    'scene_dims', 'scene_pixel_size', 'data_dimensions', 'data_elements', 'acolite_version', \n",
    "    'acolite_file_type', 'tile_code', 'proj4_string', 'pixel_size', 'uoz', 'uwv', 'wind', \n",
    "    'pressure', 'oname'\n",
    "]\n",
    "ds.attrs = {key: data.attrs[key] for key in attributes_to_keep}\n",
    "\n",
    "# Close the original dataset\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccdf360-ace7-43ac-b692-f159abee7921",
   "metadata": {},
   "source": [
    "### Reset encoding and define projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649c0bca-728d-4632-9df2-0edaa74255ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CRS: EPSG:32748\n"
     ]
    }
   ],
   "source": [
    "# Reset encoding \n",
    "ds = ds.drop_encoding()\n",
    "\n",
    "# Set CRS\n",
    "wkt = ds.attrs['proj4_string']\n",
    "ds = ds.rio.write_crs(wkt, inplace=True)\n",
    "\n",
    "# Drop 'grid_mapping'\n",
    "for var in ds.data_vars:\n",
    "    if 'grid_mapping' in ds[var].attrs:\n",
    "        del ds[var].attrs['grid_mapping']\n",
    "\n",
    "# Print the current CRS\n",
    "print(\"Current CRS:\", ds.rio.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf4584",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50314dd2",
   "metadata": {},
   "source": [
    "## Sun Glint Correction (Hedley et al., 2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bae9d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum NIR brightness (MinNir): -0.00034847320057451725\n",
      "Regression results for blue: slope=0.7068467459698609, r_value=0.49478330537082554, p_value=0.0\n",
      "Regression results for green: slope=0.789761224416469, r_value=0.579725037945171, p_value=0.0\n",
      "Regression results for red: slope=0.7772012852314969, r_value=0.6827873468967652, p_value=0.0\n",
      "Regression results for red_edge: slope=0.6594397377757661, r_value=0.673968606695982, p_value=0.0\n",
      "Slope information not found for variable 'swir1'. Skipping correction.\n",
      "Slope information not found for variable 'swir2'. Skipping correction.\n"
     ]
    }
   ],
   "source": [
    "# Subsetting sample area for the Sun Glint Correction\n",
    "# Read shapefile and desired year for the input\n",
    "var_select = ['blue', 'green', 'red', 'red_edge', 'nir']  # Variables to select from the dataset\n",
    "gdf = gpd.read_file(shapefile_path_deepWater)  # Load shapefile containing the region of interest\n",
    "desired_year = int(user_year)  # Convert user_year to integer\n",
    "\n",
    "# Mask the dataset based on the shapefile and desired year\n",
    "samples = bm.mask_dataset(\n",
    "    ds[var_select], gdf, desired_year\n",
    ")\n",
    "\n",
    "# Compute sun glint correction using the 'sunglint_correction' function from the module\n",
    "# Note: The 'vars_ignore' parameter excludes 'lat' and 'lon' from the correction process. Default set to None\n",
    "sg_ds = bm.sunglint_correction(ds, samples, 'nir', vars_ignore=['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ca576-5ad5-449b-9979-0c174975f132",
   "metadata": {},
   "source": [
    "## Depth Invariant Index (Green et al., 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc63319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating DII for bands blue_sg and green_sg with k-ratio: 0.721259206831931\n",
      "Calculating DII for bands blue_sg and red_sg with k-ratio: 0.6542580278231951\n",
      "Calculating DII for bands blue_sg and red_edge_sg with k-ratio: 0.46927000614747094\n",
      "Calculating DII for bands green_sg and red_sg with k-ratio: 0.9935577984441735\n",
      "Calculating DII for bands green_sg and red_edge_sg with k-ratio: 1.053037410596423\n",
      "Calculating DII for bands red_sg and red_edge_sg with k-ratio: 1.0408077657224628\n"
     ]
    }
   ],
   "source": [
    "# Subsetting sample area for the DII calculation\n",
    "# Read the shapefile containing the region of interest\n",
    "gdf = gpd.read_file(shapefile_path_sandObject)  # Load shapefile for sand object classification\n",
    "\n",
    "# Mask the dataset based on the shapefile and desired year\n",
    "samples = bm.mask_dataset(\n",
    "    sg_ds, gdf, desired_year\n",
    ")\n",
    "\n",
    "# Define pairs of bands for which to calculate k-ratio and Depth Invariant Index (DII)\n",
    "band_pairs = [\n",
    "    ('blue_sg', 'green_sg'),\n",
    "    ('blue_sg', 'red_sg'),\n",
    "    ('blue_sg', 'red_edge_sg'),\n",
    "    ('green_sg', 'red_sg'),\n",
    "    ('green_sg', 'red_edge_sg'),\n",
    "    ('red_sg', 'red_edge_sg')\n",
    "]\n",
    "\n",
    "# Calculate the water column corrected dataset using the specified band pairs\n",
    "wc_ds = bm.water_column_correction(sg_ds, samples, band_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05674246-a6b1-4ea4-8931-f232673c97c4",
   "metadata": {},
   "source": [
    "## Spectral Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda6920",
   "metadata": {},
   "source": [
    "### Normalized Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a7fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pairs of variables for which to compute normalized difference indices\n",
    "variable_pairs = [\n",
    "    ('nir', 'red'),\n",
    "    ('green', 'swir1'),\n",
    "    ('nir', 'green'),\n",
    "    ('green', 'red'),\n",
    "    ('green', 'red_edge')\n",
    "]\n",
    "\n",
    "# Define names for the resulting normalized difference indices\n",
    "var_names = ['ndvi', 'mndwi', 'gndvi', 'ngrdi_red','ngrdi_red_edge']\n",
    "\n",
    "# Compute the normalized difference indices\n",
    "si_ds = bm.normalized_difference(ds, variable_pairs, var_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a628d4",
   "metadata": {},
   "source": [
    "### Non-Normalized Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d51f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate EVI\n",
    "evi = (ds['nir'] - ds['red']) / (ds['nir'] + 6 * ds['red'] - 7.5 * ds['blue'] + 1)\n",
    "\n",
    "# Calculate AWEI\n",
    "awei = 4 * (ds['green'] - ds['swir2']) - (0.25 * ds['nir'] + 2.75 * ds['swir1'])\n",
    "\n",
    "# Create DataArray for EVI with attributes\n",
    "si_ds['evi'] = xr.DataArray(\n",
    "    data=evi,\n",
    "    dims=ds['nir'].dims,\n",
    "    coords=ds['nir'].coords,\n",
    "    name='evi',\n",
    "    attrs={\n",
    "        'long_name': 'Enhanced Vegetation Index (EVI)',\n",
    "        'formula': '(NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1)',\n",
    "        'units': '1',\n",
    "        'date_created': datetime.utcnow().isoformat(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create DataArray for AWEI with attributes\n",
    "si_ds['awei'] = xr.DataArray(\n",
    "    data=awei,\n",
    "    dims=ds['green'].dims,\n",
    "    coords=ds['green'].coords,\n",
    "    name='awei',\n",
    "    attrs={\n",
    "        'long_name': 'Automated Water Extraction Index (AWEI)',\n",
    "        'formula': '4 * (GREEN - SWIR2) - (0.25 * NIR + 2.75 * SWIR1)',\n",
    "        'units': '1',\n",
    "        'date_created': datetime.utcnow().isoformat(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29c8a1-5325-4339-80c9-8bddd6748f9d",
   "metadata": {},
   "source": [
    "## Merge Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e9d4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataset\n",
    "clf_ds = xr.merge([ds, sg_ds, wc_ds, si_ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c05454",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6fa2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7819155",
   "metadata": {},
   "source": [
    "## Land-Water Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96dfc8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting sample area for the classification\n",
    "# Read the shapefile containing the region of interest\n",
    "gdf = gpd.read_file(shapefile_path_training_1)\n",
    "\n",
    "# Extract labeled samples for the specified year\n",
    "samples = bm.labeled_samples(\n",
    "    clf_ds,gdf, 'class', desired_year\n",
    ")\n",
    "\n",
    "# Define the list of features to be used in the classification model\n",
    "features = [\n",
    "    'blue', 'green', 'red', 'red_edge', 'nir', 'swir1',\n",
    "    'blue_sg', 'green_sg', 'red_sg', 'red_edge_sg',\n",
    "    'dii_blue_sg_green_sg', 'dii_blue_sg_red_sg', \n",
    "    'dii_blue_sg_red_edge_sg', 'dii_green_sg_red_sg',\n",
    "    'dii_green_sg_red_edge_sg', 'dii_red_sg_red_edge_sg',\n",
    "    'mndwi', 'awei',  \n",
    "]\n",
    "\n",
    "# Prepare the feature matrix (X) and target labels (y) from the given samples\n",
    "X, y = bm.prepare_samples(samples, features, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24cf58a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>0.168497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>red_edge</td>\n",
       "      <td>0.164592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>green</td>\n",
       "      <td>0.145351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>swir1</td>\n",
       "      <td>0.059780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dii_blue_sg_green_sg</td>\n",
       "      <td>0.054943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dii_green_sg_red_edge_sg</td>\n",
       "      <td>0.051960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dii_blue_sg_red_sg</td>\n",
       "      <td>0.048199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>awei</td>\n",
       "      <td>0.047615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mndwi</td>\n",
       "      <td>0.036672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blue_sg</td>\n",
       "      <td>0.034083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dii_red_sg_red_edge_sg</td>\n",
       "      <td>0.033597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>red_edge_sg</td>\n",
       "      <td>0.029473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nir</td>\n",
       "      <td>0.028097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>green_sg</td>\n",
       "      <td>0.027143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>blue</td>\n",
       "      <td>0.026140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>red_sg</td>\n",
       "      <td>0.019816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dii_blue_sg_red_edge_sg</td>\n",
       "      <td>0.015989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dii_green_sg_red_sg</td>\n",
       "      <td>0.008053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Feature  Importance\n",
       "0                        red    0.168497\n",
       "1                   red_edge    0.164592\n",
       "2                      green    0.145351\n",
       "3                      swir1    0.059780\n",
       "4       dii_blue_sg_green_sg    0.054943\n",
       "5   dii_green_sg_red_edge_sg    0.051960\n",
       "6         dii_blue_sg_red_sg    0.048199\n",
       "7                       awei    0.047615\n",
       "8                      mndwi    0.036672\n",
       "9                    blue_sg    0.034083\n",
       "10    dii_red_sg_red_edge_sg    0.033597\n",
       "11               red_edge_sg    0.029473\n",
       "12                       nir    0.028097\n",
       "13                  green_sg    0.027143\n",
       "14                      blue    0.026140\n",
       "15                    red_sg    0.019816\n",
       "16   dii_blue_sg_red_edge_sg    0.015989\n",
       "17       dii_green_sg_red_sg    0.008053"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification procedure\n",
    "# Split the samples into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Initialiaze a Random Forest classifier\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=80, random_state=42, max_depth=15,\n",
    "    min_samples_leaf=1, min_samples_split=2\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the features importance\n",
    "importances = clf.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances\n",
    "})\n",
    "importance_df = importance_df.sort_values(\n",
    "    by='Importance', ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "# Print feature importances\n",
    "importance_df\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "export_file_path = os.path.join(out_dir, 'csv', f\"{'feature_importances_classification_1'}_{user_year}.csv\")\n",
    "importance_df.to_csv(export_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "596212f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00    157530\n",
      "           2       1.00      1.00      1.00      1527\n",
      "           3       1.00      1.00      1.00      1294\n",
      "\n",
      "    accuracy                           1.00    160351\n",
      "   macro avg       1.00      1.00      1.00    160351\n",
      "weighted avg       1.00      1.00      1.00    160351\n",
      "\n",
      "[[157529      1      0]\n",
      " [     1   1526      0]\n",
      " [     1      1   1292]]\n",
      "\n",
      "accuracy score:0.9999750547236999\n",
      "cohen's kappa:0.9992814041998561\n"
     ]
    }
   ],
   "source": [
    "# Accuracy assessment\n",
    "# Predict the labels for the test set using the trained classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the reports\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"{confusion_matrix(y_test, y_pred)}\\n\")\n",
    "print(f\"accuracy score:{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"cohen's kappa:{cohen_kappa_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d772f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict full set and reshape into the original spatial dimensions\n",
    "# Get the dimensions (height and widht) of the dataset\n",
    "height = clf_ds.sizes['y']\n",
    "width = clf_ds.sizes['x']\n",
    "\n",
    "# Stack the feature data into a 2D array 'X_full'\n",
    "# Create a mask to identify rows with valid (non-NaN) feature values\n",
    "# Predict the labels for the entire dataset using the trained RandomForest model\n",
    "X_full = np.stack([clf_ds[var].values.flatten() for var in features], axis=1)\n",
    "mask = ~np.isnan(X_full).any(axis=1)\n",
    "y_full_pred = clf.predict(X_full)\n",
    "\n",
    "# Initialize an array filled with NaN values to store reshaped predictions\n",
    "y_full_pred_reshaped = np.full((height, width), np.nan)\n",
    "# Flatten the predicted labels and create a flattened array filled with NaN values\n",
    "y_full_pred_flattened = y_full_pred.reshape(-1)\n",
    "y_full_pred_reshaped_flat = np.full(height * width, np.nan)\n",
    "# Apply the mask to place the predicted labels into the correct positions in the flattened array\n",
    "y_full_pred_reshaped_flat[mask] = y_full_pred_flattened[mask]\n",
    "# Reshape the flattened array back into the original 2D shape (height, width)\n",
    "y_full_pred_reshaped = y_full_pred_reshaped_flat.reshape((height, width))\n",
    "\n",
    "# Create a new xarray.DataArray to store the predictions in the dataset\n",
    "clf_ds['predictions'] = xr.DataArray(\n",
    "    y_full_pred_reshaped,\n",
    "    dims=clf_ds[list(clf_ds.data_vars)[0]].dims,\n",
    "    coords=clf_ds[list(clf_ds.data_vars)[0]].coords,\n",
    "    name='predictions',\n",
    "    attrs={\n",
    "        'long_name': 'Predicted label',\n",
    "        'model': 'RandomForestClassifier',\n",
    "        'data_created': datetime.utcnow().isoformat(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f9d6011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data for export\n",
    "clf_ds_export_1 = clf_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cce0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask where the predictions are equal to 2 (shallow water)\n",
    "pred_equals_2 = clf_ds['predictions'] == 2\n",
    "\n",
    "# Filter the dataset variables to include only those where the predictions equal 2\n",
    "# This retains only the values for which the condition (predictions == 2) is true\n",
    "filtered_ds = xr.Dataset({var_name: clf_ds[var_name].where(pred_equals_2) for var_name in clf_ds.data_vars})\n",
    "\n",
    "# Copy attributes from the original dataset to the new filtered dataset\n",
    "for attr in clf_ds.attrs:\n",
    "    filtered_ds.attrs[attr] = clf_ds.attrs[attr]\n",
    "\n",
    "# Copy coordinates from the original dataset to the new filtered dataset\n",
    "for coord in clf_ds.coords:\n",
    "    filtered_ds[coord] = clf_ds[coord]\n",
    "\n",
    "# Update the original dataset to the new filtered dataset\n",
    "clf_ds = filtered_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e8dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask where the predictions are equal to 2 (shallow water)\n",
    "# pred_equals_2 = clf_ds['predictions'] == 2\n",
    "pred_equals_2 = clf_ds_export_1['predictions'] == 2\n",
    "\n",
    "# Filter the dataset variables to include only those where the predictions equal 2\n",
    "# This retains only the values for which the condition (predictions == 2) is true\n",
    "filtered_ds = xr.Dataset({var_name: clf_ds_export_1[var_name].where(pred_equals_2) for var_name in clf_ds_export_1.data_vars})\n",
    "\n",
    "# Copy attributes from the original dataset to the new filtered dataset\n",
    "for attr in clf_ds_export_1.attrs:\n",
    "    filtered_ds.attrs[attr] = clf_ds_export_1.attrs[attr]\n",
    "\n",
    "# Copy coordinates from the original dataset to the new filtered dataset\n",
    "for coord in clf_ds_export_1.coords:\n",
    "    filtered_ds[coord] = clf_ds_export_1[coord]\n",
    "\n",
    "# Update the original dataset to the new filtered dataset\n",
    "clf_ds = filtered_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ab7b1",
   "metadata": {},
   "source": [
    "## Benthic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35a11501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting sample area for the classification\n",
    "# Read the shapefile containing the region of interest\n",
    "gdf = gpd.read_file(shapefile_path_training_2)\n",
    "\n",
    "# Extract labeled samples for the specified year\n",
    "samples = bm.labeled_samples(\n",
    "    clf_ds,gdf, 'class', desired_year\n",
    ")\n",
    "\n",
    "# Define the list of features to be used in the classification model\n",
    "features = [\n",
    "    'blue', 'green', 'red', 'red_edge', 'nir', 'swir1',\n",
    "    'swir2', 'blue_sg', 'green_sg', 'red_sg', 'red_edge_sg',\n",
    "    'dii_blue_sg_green_sg', 'dii_blue_sg_red_sg', \n",
    "    'dii_blue_sg_red_edge_sg', 'dii_green_sg_red_sg',\n",
    "    'dii_green_sg_red_edge_sg', 'dii_red_sg_red_edge_sg',\n",
    "    'ndvi', 'mndwi', 'gndvi', 'ngrdi_red', 'ngrdi_red_edge',\n",
    "    'evi', 'awei',  \n",
    "]\n",
    "# Prepare the feature matrix (X) and target labels (y) from the given samples\n",
    "X, y = bm.prepare_samples(samples, features, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6b6c1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>0.096661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>red_edge</td>\n",
       "      <td>0.077153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mndwi</td>\n",
       "      <td>0.070229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>red_edge_sg</td>\n",
       "      <td>0.066503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>green_sg</td>\n",
       "      <td>0.052391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>swir2</td>\n",
       "      <td>0.052356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blue_sg</td>\n",
       "      <td>0.050793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>swir1</td>\n",
       "      <td>0.050193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>green</td>\n",
       "      <td>0.047135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blue</td>\n",
       "      <td>0.043373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>red_sg</td>\n",
       "      <td>0.042743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ngrdi_red</td>\n",
       "      <td>0.041410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>awei</td>\n",
       "      <td>0.041268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>evi</td>\n",
       "      <td>0.039833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dii_green_sg_red_sg</td>\n",
       "      <td>0.035816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dii_blue_sg_red_sg</td>\n",
       "      <td>0.030739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nir</td>\n",
       "      <td>0.029545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gndvi</td>\n",
       "      <td>0.023652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dii_blue_sg_red_edge_sg</td>\n",
       "      <td>0.023221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dii_green_sg_red_edge_sg</td>\n",
       "      <td>0.019728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dii_blue_sg_green_sg</td>\n",
       "      <td>0.019309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ndvi</td>\n",
       "      <td>0.017773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ngrdi_red_edge</td>\n",
       "      <td>0.015357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dii_red_sg_red_edge_sg</td>\n",
       "      <td>0.012818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Feature  Importance\n",
       "0                        red    0.096661\n",
       "1                   red_edge    0.077153\n",
       "2                      mndwi    0.070229\n",
       "3                red_edge_sg    0.066503\n",
       "4                   green_sg    0.052391\n",
       "5                      swir2    0.052356\n",
       "6                    blue_sg    0.050793\n",
       "7                      swir1    0.050193\n",
       "8                      green    0.047135\n",
       "9                       blue    0.043373\n",
       "10                    red_sg    0.042743\n",
       "11                 ngrdi_red    0.041410\n",
       "12                      awei    0.041268\n",
       "13                       evi    0.039833\n",
       "14       dii_green_sg_red_sg    0.035816\n",
       "15        dii_blue_sg_red_sg    0.030739\n",
       "16                       nir    0.029545\n",
       "17                     gndvi    0.023652\n",
       "18   dii_blue_sg_red_edge_sg    0.023221\n",
       "19  dii_green_sg_red_edge_sg    0.019728\n",
       "20      dii_blue_sg_green_sg    0.019309\n",
       "21                      ndvi    0.017773\n",
       "22            ngrdi_red_edge    0.015357\n",
       "23    dii_red_sg_red_edge_sg    0.012818"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification procedure\n",
    "# Split the samples into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Initialiaze a Random Forest classifier\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200, random_state=42, max_depth=15,\n",
    "    min_samples_leaf=1, min_samples_split=2\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the features importance\n",
    "importances = clf.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances\n",
    "})\n",
    "importance_df = importance_df.sort_values(\n",
    "    by='Importance', ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "# Print feature importances\n",
    "importance_df\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "export_file_path = os.path.join(out_dir, 'csv', f\"{'feature_importances_classification_2'}_{user_year}.csv\")\n",
    "importance_df.to_csv(export_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b979e33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.72      0.77       144\n",
      "           2       0.94      0.98      0.96       343\n",
      "           3       0.93      0.99      0.96       323\n",
      "           4       0.96      0.93      0.94       490\n",
      "\n",
      "    accuracy                           0.93      1300\n",
      "   macro avg       0.92      0.90      0.91      1300\n",
      "weighted avg       0.93      0.93      0.93      1300\n",
      "\n",
      "[[103  14  10  17]\n",
      " [  5 336   0   2]\n",
      " [  0   1 321   1]\n",
      " [ 14   6  16 454]]\n",
      "\n",
      "accuracy score:0.9338461538461539\n",
      "cohen's kappa:0.9072691682425932\n"
     ]
    }
   ],
   "source": [
    "# Accuracy assessment\n",
    "# Predict the labels for the test set using the trained classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the reports\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"{confusion_matrix(y_test, y_pred)}\\n\")\n",
    "print(f\"accuracy score:{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"cohen's kappa:{cohen_kappa_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b470c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict full set and reshape into the original spatial dimensions\n",
    "# Get the dimensions (height and widht) of the dataset\n",
    "height = clf_ds.sizes['y']\n",
    "width = clf_ds.sizes['x']\n",
    "\n",
    "# Stack the feature data into a 2D array 'X_full'\n",
    "# Create a mask to identify rows with valid (non-NaN) feature values\n",
    "# Predict the labels for the entire dataset using the trained RandomForest model\n",
    "X_full = np.stack([clf_ds[var].values.flatten() for var in features], axis=1)\n",
    "mask = ~np.isnan(X_full).any(axis=1)\n",
    "y_full_pred = clf.predict(X_full)\n",
    "\n",
    "# Initialize an array filled with NaN values to store reshaped predictions\n",
    "y_full_pred_reshaped = np.full((height, width), np.nan)\n",
    "# Flatten the predicted labels and create a flattened array filled with NaN values\n",
    "y_full_pred_flattened = y_full_pred.reshape(-1)\n",
    "y_full_pred_reshaped_flat = np.full(height * width, np.nan)\n",
    "# Apply the mask to place the predicted labels into the correct positions in the flattened array\n",
    "y_full_pred_reshaped_flat[mask] = y_full_pred_flattened[mask]\n",
    "# Reshape the flattened array back into the original 2D shape (height, width)\n",
    "y_full_pred_reshaped = y_full_pred_reshaped_flat.reshape((height, width))\n",
    "\n",
    "# Create a new xarray.DataArray to store the predictions in the dataset\n",
    "clf_ds['predictions'] = xr.DataArray(\n",
    "    y_full_pred_reshaped,\n",
    "    dims=clf_ds[list(clf_ds.data_vars)[0]].dims,\n",
    "    coords=clf_ds[list(clf_ds.data_vars)[0]].coords,\n",
    "    name='predictions',\n",
    "    attrs={\n",
    "        'long_name': 'Predicted label',\n",
    "        'model': 'RandomForestClassifier',\n",
    "        'data_created': datetime.utcnow().isoformat(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d1194",
   "metadata": {},
   "source": [
    "# Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6c23e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_coordinate(dataset, base_filename):\n",
    "    \n",
    "    # Split the base filename to extract date and time components\n",
    "    parts = base_filename.split('_')\n",
    "\n",
    "    # Extract year, month, day, hour, and minute from the filename\n",
    "    year = int(parts[2])\n",
    "    month = int(parts[3])\n",
    "    day = int(parts[4])\n",
    "    hour = int(parts[5])\n",
    "    minute = int(parts[6])\n",
    "\n",
    "    # Create a datetime object for the time coordinate\n",
    "    time_coord = [datetime(year, month, day, hour, minute)]\n",
    "\n",
    "    # Assign the time coordinate to the dataset's coordinates\n",
    "    dataset = dataset.assign_coords(time=('time', time_coord))\n",
    "\n",
    "    # Set attributes for the 'time' coordinate\n",
    "    dataset['time'].attrs = {\n",
    "        'standard_name': 'time',\n",
    "        'long_name': 'time',\n",
    "    }\n",
    "\n",
    "    # Expand dimensions for all data variables to include the 'time' dimension\n",
    "    for var in dataset.data_vars:\n",
    "        dataset[var] = dataset[var].expand_dims('time', axis=0)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a855464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_encoding(dataset):\n",
    "    encoding = {}\n",
    "    \n",
    "    for var_name in dataset.data_vars:\n",
    "        var = dataset[var_name]\n",
    "        encoding[var_name] = {\n",
    "            \"dtype\": \"float32\",\n",
    "            'zlib': False,\n",
    "            'shuffle': False,\n",
    "            'complevel': 0,\n",
    "            'fletcher32': False,\n",
    "            'contiguous': True,\n",
    "            'chunksizes': None,\n",
    "            'original_shape': (2343, 2530),\n",
    "            'grid_mapping': 'transverse_mercator'\n",
    "        }\n",
    "        \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2832d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a new NetCDF file\n",
    "encoding = generate_encoding(clf_ds_export_1)\n",
    "clf_ds_export_1 = add_time_coordinate(clf_ds_export_1, base_filename)\n",
    "export_file_path = os.path.join(out_dir, 'processed', f\"{base_filename}_{'L2F'}.nc\")\n",
    "clf_ds_export_1.to_netcdf(export_file_path, encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e7ff4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a new NetCDF file\n",
    "encoding = generate_encoding(clf_ds)\n",
    "clf_ds = add_time_coordinate(clf_ds, base_filename)\n",
    "export_file_path = os.path.join(out_dir, 'classification', f\"{base_filename}_{'L3W'}.nc\")\n",
    "clf_ds.to_netcdf(export_file_path, encoding=encoding )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
